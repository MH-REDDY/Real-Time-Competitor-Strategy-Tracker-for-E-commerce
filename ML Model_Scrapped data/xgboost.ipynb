{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "651d1c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#@title Cell A ‚Äî Install required libs (run once)\n",
    "!pip install xgboost scikit-learn pandas tqdm joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb47af19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kvpra\\onedrive\\desktop\\sample_project_1\\myenv1\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Scraped docs: 270\n",
      "Synthetic docs: 1365\n",
      "‚úÖ Combined dataset loaded successfully! Rows: 1635, ASINs: 5\n",
      "Scraped docs: 270\n",
      "Synthetic docs: 1365\n",
      "‚úÖ Combined dataset loaded successfully! Rows: 1635, ASINs: 5\n"
     ]
    }
   ],
   "source": [
    "#@title Cell A ‚Äî Install required libs (run once)\n",
    "!pip install xgboost scikit-learn pandas tqdm joblib\n",
    "\n",
    "# ================================================\n",
    "#  XGBOOST PRICE FORECASTING PIPELINE (VS CODE)\n",
    "# ================================================\n",
    "\n",
    "# === Imports ===\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from joblib import dump, load\n",
    "\n",
    "# Reproducible randomness for model training & forecast generation\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# ================================================\n",
    "#  CELL B ‚Äî CONNECT TO MONGODB AND LOAD DATA\n",
    "# ================================================\n",
    "\n",
    "load_dotenv()\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\")\n",
    "if not MONGO_URI:\n",
    "    raise ValueError(\"‚ùå MONGO_URI not found. Check your .env file.\")\n",
    "\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[\"ecom_tracker\"]\n",
    "\n",
    "scraped_coll = db[\"price_history\"]\n",
    "synthetic_coll = db[\"Sythetic data 2\"]\n",
    "\n",
    "scraped_docs = list(scraped_coll.find({}))\n",
    "synthetic_docs = list(synthetic_coll.find({}))\n",
    "\n",
    "print(f\"Scraped docs: {len(scraped_docs)}\")\n",
    "print(f\"Synthetic docs: {len(synthetic_docs)}\")\n",
    "\n",
    "df_scraped = pd.DataFrame(scraped_docs)\n",
    "df_synth = pd.DataFrame(synthetic_docs)\n",
    "\n",
    "for df_temp in (df_scraped, df_synth):\n",
    "    if \"scraped_at\" in df_temp.columns:\n",
    "        df_temp[\"scraped_at\"] = pd.to_datetime(df_temp[\"scraped_at\"], utc=True)\n",
    "    for col in [\"price\", \"original_price\", \"discount_percent\"]:\n",
    "        if col in df_temp.columns:\n",
    "            df_temp[col] = pd.to_numeric(df_temp[col], errors=\"coerce\")\n",
    "\n",
    "df = pd.concat([df_scraped, df_synth], ignore_index=True, sort=False)\n",
    "df = df[[\"asin\", \"price\", \"original_price\", \"discount_percent\", \"scraped_at\"]]\n",
    "df = df.dropna(subset=[\"asin\", \"price\", \"scraped_at\"]).sort_values([\"asin\", \"scraped_at\"])\n",
    "print(f\"‚úÖ Combined dataset loaded successfully! Rows: {len(df)}, ASINs: {df['asin'].nunique()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd8f1f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Feature dataframe ready! Shape: (1635, 32)\n",
      "         asin    price  original_price  discount_percent  \\\n",
      "0  B09CYX92NB  9842.62          9999.0              1.56   \n",
      "1  B09CYX92NB  9999.00          9999.0              0.00   \n",
      "2  B09CYX92NB  9983.33          9999.0              0.16   \n",
      "3  B09CYX92NB  9637.54          9999.0              3.61   \n",
      "4  B09CYX92NB  9593.19          9999.0              4.06   \n",
      "\n",
      "                 scraped_at  dayofweek  day  month  hour  is_weekend  ...  \\\n",
      "0 2025-08-04 09:49:00+00:00          0    4      8     9           0  ...   \n",
      "1 2025-08-04 09:50:00+00:00          0    4      8     9           0  ...   \n",
      "2 2025-08-04 09:51:00+00:00          0    4      8     9           0  ...   \n",
      "3 2025-08-05 17:15:00+00:00          1    5      8    17           0  ...   \n",
      "4 2025-08-05 17:16:00+00:00          1    5      8    17           0  ...   \n",
      "\n",
      "   lag_pct_9  lag_price_21  lag_pct_21  lag_price_42  lag_pct_42  roll_mean_3  \\\n",
      "0    0.98436       9842.62     0.98436       9842.62     0.98436      9842.62   \n",
      "1    0.98436       9842.62     0.98436       9842.62     0.98436      9842.62   \n",
      "2    0.98436       9842.62     0.98436       9842.62     0.98436      9920.81   \n",
      "3    0.98436       9842.62     0.98436       9842.62     0.98436      9941.65   \n",
      "4    0.98436       9842.62     0.98436       9842.62     0.98436      9873.29   \n",
      "\n",
      "   roll_std_3  roll_mean_9  roll_mean_27  pct_change_3  \n",
      "0    0.000000    9842.6200     9842.6200      0.000000  \n",
      "1    0.000000    9842.6200     9842.6200      0.000000  \n",
      "2  110.577358    9920.8100     9920.8100      0.000000  \n",
      "3   86.119643    9941.6500     9941.6500      0.000000  \n",
      "4  204.315771    9865.6225     9865.6225     -0.020836  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "# --- Cell C: Feature Engineering (clean version, no warnings) ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import random\n",
    "\n",
    "# Define Amazon event windows in UTC\n",
    "def ist_to_utc(year, month, day, hour=0, minute=0):\n",
    "    return datetime(year, month, day, hour, minute,\n",
    "                    tzinfo=timezone(timedelta(hours=5, minutes=30))\n",
    "                    ).astimezone(timezone.utc)\n",
    "\n",
    "# Amazon sale events (for 2025)\n",
    "GIF_START = ist_to_utc(2025, 9, 23, 0, 0)\n",
    "GIF_END   = ist_to_utc(2025, 10, 20, 23, 59)\n",
    "PRIME_START = ist_to_utc(2025, 7, 12, 0, 0)\n",
    "PRIME_END   = ist_to_utc(2025, 7, 14, 23, 59)\n",
    "\n",
    "def is_gif(dt): return (dt >= GIF_START) and (dt <= GIF_END)\n",
    "def is_prime(dt): return (dt >= PRIME_START) and (dt <= PRIME_END)\n",
    "\n",
    "def make_features_for_group(g):\n",
    "    g = g.sort_values(\"scraped_at\").copy()\n",
    "    g[\"scraped_at\"] = pd.to_datetime(g[\"scraped_at\"], utc=True)\n",
    "    g = g.reset_index(drop=True)\n",
    "\n",
    "    # Basic time features\n",
    "    g[\"dayofweek\"] = g[\"scraped_at\"].dt.weekday\n",
    "    g[\"day\"] = g[\"scraped_at\"].dt.day\n",
    "    g[\"month\"] = g[\"scraped_at\"].dt.month\n",
    "    g[\"hour\"] = g[\"scraped_at\"].dt.hour\n",
    "    g[\"is_weekend\"] = (g[\"dayofweek\"] >= 5).astype(int)\n",
    "    g[\"is_gif\"] = g[\"scraped_at\"].apply(lambda x: 1 if is_gif(x.to_pydatetime()) else 0)\n",
    "    g[\"is_prime\"] = g[\"scraped_at\"].apply(lambda x: 1 if is_prime(x.to_pydatetime()) else 0)\n",
    "    g[\"price_to_mrp\"] = g[\"price\"] / g[\"original_price\"]\n",
    "\n",
    "    # Lag features\n",
    "    lag_steps = [1, 2, 3, 6, 9, 21, 42]\n",
    "    for L in lag_steps:\n",
    "        g[f\"lag_price_{L}\"] = g[\"price\"].shift(L)\n",
    "        g[f\"lag_pct_{L}\"] = g[\"price_to_mrp\"].shift(L)\n",
    "\n",
    "    # Rolling statistics\n",
    "    g[\"roll_mean_3\"] = g[\"price\"].rolling(3, min_periods=1).mean().shift(1)\n",
    "    g[\"roll_std_3\"] = g[\"price\"].rolling(3, min_periods=1).std().shift(1).fillna(0)\n",
    "    g[\"roll_mean_9\"] = g[\"price\"].rolling(9, min_periods=1).mean().shift(1)\n",
    "    g[\"roll_mean_27\"] = g[\"price\"].rolling(27, min_periods=1).mean().shift(1)\n",
    "    g[\"pct_change_3\"] = g[\"price\"].pct_change(3).shift(1).fillna(0)\n",
    "\n",
    "    # Fill missing values: prefer forward-fill to avoid leaking future values into past rows.\n",
    "    # Use back-fill only to fill any initial rows if absolutely necessary.\n",
    "    g = g.ffill().bfill()  # Prefer forward fill first to avoid leaking future data\n",
    "\n",
    "    return g\n",
    "\n",
    "# Apply feature generation per ASIN\n",
    "groups = [make_features_for_group(g) for _, g in df.groupby(\"asin\")]\n",
    "df_feat = pd.concat(groups, ignore_index=True, sort=False)\n",
    "\n",
    "print(f\"‚úÖ Feature dataframe ready! Shape: {df_feat.shape}\")\n",
    "print(df_feat.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "208ad464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "#  CELL D ‚Äî METRICS FUNCTION\n",
    "# ================================================\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / np.where(y_true == 0, 1, y_true))) * 100\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE(%)\": mape}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2a8a10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns: ['dayofweek', 'day', 'month', 'hour', 'is_weekend', 'is_gif', 'is_prime', 'price_to_mrp', 'lag_price_1', 'lag_pct_1', 'lag_price_2', 'lag_pct_2', 'lag_price_3', 'lag_pct_3', 'lag_price_6', 'lag_pct_6', 'lag_price_9', 'lag_pct_9', 'lag_price_21', 'lag_pct_21', 'lag_price_42', 'lag_pct_42', 'roll_mean_3', 'roll_std_3', 'roll_mean_9', 'roll_mean_27', 'pct_change_3', 'asin_encoded']\n",
      "‚úÖ Global XGBoost model trained and saved as 'model_bundle.joblib' (model + metadata)!\n",
      "‚úÖ Global XGBoost model trained and saved as 'model_bundle.joblib' (model + metadata)!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from joblib import dump\n",
    "import numpy as np\n",
    "\n",
    "# Label encode ASINs\n",
    "asin_le = LabelEncoder()\n",
    "df_feat[\"asin_encoded\"] = asin_le.fit_transform(df_feat[\"asin\"])\n",
    "\n",
    "# Features now include product code\n",
    "exclude_cols = [\"asin\", \"price\", \"original_price\", \"discount_percent\", \"scraped_at\"]\n",
    "feature_cols = [c for c in df_feat.columns if c not in exclude_cols]\n",
    "print(\"Feature columns:\", feature_cols)\n",
    "\n",
    "# Prepare data\n",
    "df_feat = df_feat.sort_values(\"scraped_at\")\n",
    "X = df_feat[feature_cols].values\n",
    "y = df_feat[\"price\"].values\n",
    "\n",
    "# Time-series split (on overall time, not per ASIN)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model_global = XGBRegressor(\n",
    "    n_estimators=800,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=2,\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Train (optionally loop through CV manually to evaluate)\n",
    "model_global.fit(X, y)\n",
    "\n",
    "# Save model + metadata (feature columns and label encoder) together so it can be loaded reproducibly\n",
    "from joblib import dump\n",
    "\n",
    "dump({'model': model_global, 'feature_cols': feature_cols, 'asin_le': asin_le}, \"model_bundle.joblib\")\n",
    "print(\"‚úÖ Global XGBoost model trained and saved as 'model_bundle.joblib' (model + metadata)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a774f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from 'model_bundle.joblib'\n",
      "Forecasting ASIN: B09CYX92NB\n",
      "Forecasting ASIN: B0C3HCD34R\n",
      "Forecasting ASIN: B0C3HCD34R\n",
      "Forecasting ASIN: B0DG2SLR9F\n",
      "Forecasting ASIN: B0DG2SLR9F\n",
      "Forecasting ASIN: B0DHKJ5HWL\n",
      "Forecasting ASIN: B0DHKJ5HWL\n",
      "Forecasting ASIN: B0DV5HX4JZ\n",
      "Forecasting ASIN: B0DV5HX4JZ\n",
      "\n",
      "‚úÖ Global model forecast complete! Rows: 450\n",
      "         asin  predicted_price  predicted_discount_percent  \\\n",
      "0  B09CYX92NB      5994.602539                       40.05   \n",
      "1  B09CYX92NB      5994.561523                       40.05   \n",
      "2  B09CYX92NB      5994.561523                       40.05   \n",
      "3  B09CYX92NB      5993.985840                       40.05   \n",
      "4  B09CYX92NB      5993.980469                       40.05   \n",
      "\n",
      "                forecast_ts  \n",
      "0 2025-11-07 20:00:00+00:00  \n",
      "1 2025-11-07 20:01:00+00:00  \n",
      "2 2025-11-07 20:02:00+00:00  \n",
      "3 2025-11-08 09:00:00+00:00  \n",
      "4 2025-11-08 09:01:00+00:00  \n",
      "\n",
      "‚úÖ Global model forecast complete! Rows: 450\n",
      "         asin  predicted_price  predicted_discount_percent  \\\n",
      "0  B09CYX92NB      5994.602539                       40.05   \n",
      "1  B09CYX92NB      5994.561523                       40.05   \n",
      "2  B09CYX92NB      5994.561523                       40.05   \n",
      "3  B09CYX92NB      5993.985840                       40.05   \n",
      "4  B09CYX92NB      5993.980469                       40.05   \n",
      "\n",
      "                forecast_ts  \n",
      "0 2025-11-07 20:00:00+00:00  \n",
      "1 2025-11-07 20:01:00+00:00  \n",
      "2 2025-11-07 20:02:00+00:00  \n",
      "3 2025-11-08 09:00:00+00:00  \n",
      "4 2025-11-08 09:01:00+00:00  \n"
     ]
    }
   ],
   "source": [
    "# --- Cell F: 30-Day Forecast using Global Model ---\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# Load trained model + label encoder (support bundle + legacy files)\n",
    "try:\n",
    "    bundle = load(\"model_bundle.joblib\")\n",
    "    if isinstance(bundle, dict):\n",
    "        model_global = bundle.get(\"model\", bundle)\n",
    "        asin_le = bundle.get(\"asin_le\", None)\n",
    "        # if feature_cols are stored in the bundle, use them; otherwise keep existing\n",
    "        try:\n",
    "            feature_cols = bundle.get(\"feature_cols\", feature_cols)\n",
    "        except NameError:\n",
    "            feature_cols = bundle.get(\"feature_cols\")\n",
    "    else:\n",
    "        # legacy single-object file\n",
    "        model_global = bundle\n",
    "        try:\n",
    "            asin_le = load(\"asin_labelencoder.joblib\")\n",
    "        except Exception:\n",
    "            asin_le = None\n",
    "    print(\"Loaded model from 'model_bundle.joblib'\")\n",
    "except FileNotFoundError:\n",
    "    # Fallback for older notebooks / artifacts\n",
    "    print(\"model_bundle.joblib not found, falling back to legacy files\")\n",
    "    model_global = load(\"xgb_global.joblib\")\n",
    "    try:\n",
    "        asin_le = load(\"asin_labelencoder.joblib\")\n",
    "    except Exception:\n",
    "        asin_le = None\n",
    "\n",
    "forecast_rows = []\n",
    "\n",
    "def generate_future_timestamps(last_ts, days=30, samples_per_day=3):\n",
    "    \"\"\"Generate timestamps for next 30 days with slight time variations.\"\"\"\n",
    "    future = []\n",
    "    cur_date = (last_ts + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "    for _ in range(days):\n",
    "        base_hour = np.random.choice([9, 14, 20])  # 3 typical scraping times\n",
    "        for i in range(samples_per_day):\n",
    "            ts = cur_date.replace(hour=base_hour, minute=i)\n",
    "            ts = pd.to_datetime(ts)\n",
    "            if ts.tzinfo is None or ts.tzinfo.utcoffset(ts) is None:\n",
    "                ts_utc = ts.tz_localize(\"UTC\")\n",
    "            else:\n",
    "                ts_utc = ts.tz_convert(\"UTC\")\n",
    "            future.append(ts_utc)\n",
    "        cur_date += timedelta(days=1)\n",
    "    return future\n",
    "\n",
    "\n",
    "# Forecast for each ASIN\n",
    "for asin, g in df_feat.groupby(\"asin\"):\n",
    "    print(f\"Forecasting ASIN: {asin}\")\n",
    "    if asin_le is None:\n",
    "        raise RuntimeError(\"Label encoder (asin_le) not available. Ensure model_bundle.joblib contains 'asin_le' or provide 'asin_labelencoder.joblib'.\")\n",
    "\n",
    "    asin_code = asin_le.transform([asin])[0]\n",
    "    g = g.sort_values(\"scraped_at\").reset_index(drop=True)\n",
    "    last_ts = g[\"scraped_at\"].iloc[-1].to_pydatetime()\n",
    "\n",
    "    future_ts = generate_future_timestamps(last_ts, 30, 3)\n",
    "    rolling = g.copy()\n",
    "\n",
    "    for ts in future_ts:\n",
    "        new_row = {\n",
    "            \"asin\": asin,\n",
    "            \"asin_encoded\": asin_code,\n",
    "            \"scraped_at\": ts,\n",
    "            \"original_price\": rolling[\"original_price\"].iloc[-1],\n",
    "        }\n",
    "\n",
    "        # Date/time features\n",
    "        new_row[\"dayofweek\"] = ts.weekday()\n",
    "        new_row[\"day\"], new_row[\"month\"], new_row[\"hour\"] = ts.day, ts.month, ts.hour\n",
    "        new_row[\"is_weekend\"] = 1 if ts.weekday() >= 5 else 0\n",
    "        new_row[\"is_gif\"], new_row[\"is_prime\"] = int(is_gif(ts)), int(is_prime(ts))\n",
    "\n",
    "        # Price relationships\n",
    "        new_row[\"price_to_mrp\"] = rolling[\"price\"].iloc[-1] / new_row[\"original_price\"]\n",
    "\n",
    "        # Lag features\n",
    "        for L in [1, 2, 3, 6, 9, 21, 42]:\n",
    "            val = rolling[\"price\"].iloc[-L] if len(rolling) >= L else rolling[\"price\"].iloc[0]\n",
    "            new_row[f\"lag_price_{L}\"] = val\n",
    "            new_row[f\"lag_pct_{L}\"] = val / new_row[\"original_price\"]\n",
    "\n",
    "        # Rolling stats\n",
    "        arr = rolling[\"price\"].values\n",
    "        new_row[\"roll_mean_3\"]  = np.mean(arr[-3:])\n",
    "        new_row[\"roll_mean_9\"]  = np.mean(arr[-9:])\n",
    "        new_row[\"roll_mean_27\"] = np.mean(arr[-27:])\n",
    "        new_row[\"roll_std_3\"]   = np.std(arr[-3:])\n",
    "        new_row[\"pct_change_3\"] = (arr[-1] - arr[-4]) / arr[-4] if len(arr) > 4 else 0\n",
    "\n",
    "        # Predict next price\n",
    "        X_new = pd.DataFrame([new_row])[feature_cols].values\n",
    "        pred = float(model_global.predict(X_new)[0])\n",
    "        pred = max(0, min(pred, new_row[\"original_price\"]))\n",
    "\n",
    "        new_row[\"price\"] = pred\n",
    "        new_row[\"discount_percent\"] = round((1 - pred / new_row[\"original_price\"]) * 100, 2)\n",
    "\n",
    "        forecast_rows.append({\n",
    "            \"asin\": asin,\n",
    "            \"predicted_price\": pred,\n",
    "            \"predicted_discount_percent\": new_row[\"discount_percent\"],\n",
    "            \"forecast_ts\": ts\n",
    "        })\n",
    "\n",
    "        # Append prediction to maintain continuity\n",
    "        rolling = pd.concat([rolling, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_forecast = pd.DataFrame(forecast_rows)\n",
    "print(f\"\\n‚úÖ Global model forecast complete! Rows: {len(df_forecast)}\")\n",
    "print(df_forecast.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccdbd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Optional: Save forecast to MongoDB ---\n",
    "# forecast_coll = db[\"price_forecast_30d\"]\n",
    "# forecast_coll.insert_many(df_forecast.to_dict(\"records\"))\n",
    "# print(f\"‚úÖ Forecast saved to MongoDB collection 'price_forecast_30d'\")\n",
    "\n",
    "# # --- Optional: Save to CSV ---\n",
    "# df_forecast.to_csv(\"30_day_forecast_global.csv\", index=False)\n",
    "# print(\"üìÅ Forecast also saved as '30_day_forecast_global.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7d5590e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Model Evaluation Summary (Global XGBoost):\n",
      "      ASIN      MAE     RMSE  MAPE(%)\n",
      "B09CYX92NB 0.120466 0.170969 0.001534\n",
      "B0DV5HX4JZ 0.171329 0.256970 0.002784\n",
      "B0C3HCD34R 0.175896 0.245648 0.003564\n",
      "B0DHKJ5HWL 0.125020 0.175357 0.004533\n",
      "B0DG2SLR9F 0.159918 0.230151 0.004982\n",
      "\n",
      "‚úÖ Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# --- Cell H: Evaluation Summary (for Global Model) ---\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_global_model(df_feat, model):\n",
    "    \"\"\"Compute evaluation metrics overall and per ASIN.\"\"\"\n",
    "    X = df_feat[feature_cols].values\n",
    "    y_true = df_feat[\"price\"].values\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    results = []\n",
    "    for asin, g in df_feat.groupby(\"asin\"):\n",
    "        Xg, yg = g[feature_cols].values, g[\"price\"].values\n",
    "        yg_pred = model.predict(Xg)\n",
    "        mae = mean_absolute_error(yg, yg_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(yg, yg_pred))\n",
    "        denom = np.where(yg == 0, 1, yg)\n",
    "        mape = np.mean(np.abs((yg - yg_pred) / denom)) * 100\n",
    "        results.append((asin, mae, rmse, mape))\n",
    "    \n",
    "    df_eval = pd.DataFrame(results, columns=[\"ASIN\", \"MAE\", \"RMSE\", \"MAPE(%)\"])\n",
    "    df_eval = df_eval.sort_values(\"MAPE(%)\")\n",
    "    return df_eval\n",
    "\n",
    "# Evaluate\n",
    "# If model was saved as a bundle, load appropriately. For example:\n",
    "# from joblib import load\n",
    "# bundle = load(\"model_bundle.joblib\")\n",
    "# model_global = bundle['model']\n",
    "\n",
    "try:\n",
    "    df_eval = evaluate_global_model(df_feat, model_global)\n",
    "    print(\"üìä Model Evaluation Summary (Global XGBoost):\")\n",
    "    print(df_eval.to_string(index=False))\n",
    "    print(\"\\n‚úÖ Evaluation complete!\")\n",
    "except Exception as e:\n",
    "    print(\"Evaluation failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a677067a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running per-ASIN holdout evaluation with 14 days holdout...\n",
      "Train rows: 1200, Test rows: 435, ASINs in test after filtering: 5\n",
      "Training global model on train split...\n",
      "Training complete.\n",
      "\n",
      "Holdout summary:\n",
      "ASINs evaluated: 5\n",
      "Median WMAPE: 0.0607\n",
      "75th percentile WMAPE: 0.0798\n",
      "Fraction ASINs WMAPE <10%: 80.00%\n",
      "Fraction ASINs WMAPE <20%: 80.00%\n",
      "Fraction improved over naive baseline: 100.00%\n",
      "Saved per-ASIN holdout evaluation to holdout_eval.csv\n",
      "\n",
      "Top 5 ASINs by best WMAPE:\n",
      "      ASIN        MAE       RMSE   MAPE(%)    WMAPE   MAE_naive  WMAPE_naive  n_test_rows\n",
      "B09CYX92NB 142.489406 169.413676  2.264362 0.019561 1982.235930     0.272126           86\n",
      "B0C3HCD34R  95.875705 132.432817  2.121054 0.019992  761.457356     0.158776           87\n",
      "B0DV5HX4JZ 295.909177 564.515459 10.379373 0.060744 3456.558046     0.709556           87\n",
      "B0DG2SLR9F 224.533314 287.463910  9.758607 0.079788  890.413448     0.316411           87\n",
      "B0DHKJ5HWL 773.720662 965.418119 58.983223 0.364244 1341.798068     0.631678           88\n",
      "\n",
      "Top 5 ASINs by worst WMAPE:\n",
      "      ASIN        MAE       RMSE   MAPE(%)    WMAPE   MAE_naive  WMAPE_naive  n_test_rows\n",
      "B09CYX92NB 142.489406 169.413676  2.264362 0.019561 1982.235930     0.272126           86\n",
      "B0C3HCD34R  95.875705 132.432817  2.121054 0.019992  761.457356     0.158776           87\n",
      "B0DV5HX4JZ 295.909177 564.515459 10.379373 0.060744 3456.558046     0.709556           87\n",
      "B0DG2SLR9F 224.533314 287.463910  9.758607 0.079788  890.413448     0.316411           87\n",
      "B0DHKJ5HWL 773.720662 965.418119 58.983223 0.364244 1341.798068     0.631678           88\n",
      "Training complete.\n",
      "\n",
      "Holdout summary:\n",
      "ASINs evaluated: 5\n",
      "Median WMAPE: 0.0607\n",
      "75th percentile WMAPE: 0.0798\n",
      "Fraction ASINs WMAPE <10%: 80.00%\n",
      "Fraction ASINs WMAPE <20%: 80.00%\n",
      "Fraction improved over naive baseline: 100.00%\n",
      "Saved per-ASIN holdout evaluation to holdout_eval.csv\n",
      "\n",
      "Top 5 ASINs by best WMAPE:\n",
      "      ASIN        MAE       RMSE   MAPE(%)    WMAPE   MAE_naive  WMAPE_naive  n_test_rows\n",
      "B09CYX92NB 142.489406 169.413676  2.264362 0.019561 1982.235930     0.272126           86\n",
      "B0C3HCD34R  95.875705 132.432817  2.121054 0.019992  761.457356     0.158776           87\n",
      "B0DV5HX4JZ 295.909177 564.515459 10.379373 0.060744 3456.558046     0.709556           87\n",
      "B0DG2SLR9F 224.533314 287.463910  9.758607 0.079788  890.413448     0.316411           87\n",
      "B0DHKJ5HWL 773.720662 965.418119 58.983223 0.364244 1341.798068     0.631678           88\n",
      "\n",
      "Top 5 ASINs by worst WMAPE:\n",
      "      ASIN        MAE       RMSE   MAPE(%)    WMAPE   MAE_naive  WMAPE_naive  n_test_rows\n",
      "B09CYX92NB 142.489406 169.413676  2.264362 0.019561 1982.235930     0.272126           86\n",
      "B0C3HCD34R  95.875705 132.432817  2.121054 0.019992  761.457356     0.158776           87\n",
      "B0DV5HX4JZ 295.909177 564.515459 10.379373 0.060744 3456.558046     0.709556           87\n",
      "B0DG2SLR9F 224.533314 287.463910  9.758607 0.079788  890.413448     0.316411           87\n",
      "B0DHKJ5HWL 773.720662 965.418119 58.983223 0.364244 1341.798068     0.631678           88\n"
     ]
    }
   ],
   "source": [
    "# --- Cell I: Out-of-sample per-ASIN holdout evaluation (14 days) ---\n",
    "# This cell performs a time-based holdout (last 14 days per ASIN), trains a global model on the remaining\n",
    "# data, and evaluates model vs a naive baseline. Results are saved to `holdout_eval.csv`.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "holdout_days = 14\n",
    "print(f\"Running per-ASIN holdout evaluation with {holdout_days} days holdout...\")\n",
    "\n",
    "# Build test index: last `holdout_days` for each ASIN\n",
    "test_idx = []\n",
    "for asin, g in df_feat.groupby('asin'):\n",
    "    last_ts = g['scraped_at'].max()\n",
    "    cutoff = last_ts - timedelta(days=holdout_days)\n",
    "    idx = g[g['scraped_at'] > cutoff].index.tolist()\n",
    "    if len(idx) > 0:\n",
    "        test_idx.extend(idx)\n",
    "\n",
    "if len(test_idx) == 0:\n",
    "    raise RuntimeError('No test rows found for chosen holdout_days. Increase dataset or reduce holdout_days.')\n",
    "\n",
    "# Create train/test splits\n",
    "df_test = df_feat.loc[sorted(test_idx)].copy()\n",
    "df_train = df_feat.drop(index=test_idx).copy()\n",
    "\n",
    "# Remove ASINs with no training data\n",
    "asins_with_train = set(df_train['asin'].unique())\n",
    "df_test = df_test[df_test['asin'].isin(asins_with_train)].copy()\n",
    "\n",
    "print(f\"Train rows: {len(df_train)}, Test rows: {len(df_test)}, ASINs in test after filtering: {df_test['asin'].nunique()}\")\n",
    "\n",
    "# Prepare label encoder and encoded column consistent with training\n",
    "le = LabelEncoder()\n",
    "le.fit(df_train['asin'])\n",
    "df_train['asin_encoded'] = le.transform(df_train['asin'])\n",
    "df_test['asin_encoded'] = le.transform(df_test['asin'])\n",
    "\n",
    "# Feature columns (same exclude rule as training)\n",
    "exclude_cols = [\"asin\", \"price\", \"original_price\", \"discount_percent\", \"scraped_at\"]\n",
    "feature_cols_holdout = [c for c in df_train.columns if c not in exclude_cols]\n",
    "\n",
    "# Train global model on df_train\n",
    "X_train = df_train[feature_cols_holdout].values\n",
    "y_train = df_train['price'].values\n",
    "\n",
    "model_holdout = XGBRegressor(\n",
    "    n_estimators=800,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=2,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print('Training global model on train split...')\n",
    "model_holdout.fit(X_train, y_train)\n",
    "print('Training complete.')\n",
    "\n",
    "# Predict on test set\n",
    "X_test = df_test[feature_cols_holdout].values\n",
    "y_test = df_test['price'].values\n",
    "\n",
    "y_pred = model_holdout.predict(X_test)\n",
    "\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_test['pred'] = y_pred\n",
    "\n",
    "# Naive baseline: last observed price from training for that ASIN\n",
    "last_price_map = df_train.groupby('asin')['price'].last().to_dict()\n",
    "\n",
    "def naively_predict(row):\n",
    "    return last_price_map.get(row['asin'], df_train['price'].median())\n",
    "\n",
    "df_test['naive_pred'] = df_test.apply(naively_predict, axis=1)\n",
    "\n",
    "# Compute per-ASIN metrics\n",
    "results = []\n",
    "for asin, g in df_test.groupby('asin'):\n",
    "    yg = g['price'].values\n",
    "    yp = g['pred'].values\n",
    "    yb = g['naive_pred'].values\n",
    "    mae = mean_absolute_error(yg, yp)\n",
    "    rmse = np.sqrt(mean_squared_error(yg, yp))\n",
    "    denom = np.where(yg == 0, 1, yg)\n",
    "    mape = np.mean(np.abs((yg - yp) / denom)) * 100\n",
    "    wmape = np.sum(np.abs(yg - yp)) / np.sum(np.abs(yg)) if np.sum(np.abs(yg)) > 0 else np.nan\n",
    "\n",
    "    # Baseline errors (naive)\n",
    "    mae_base = mean_absolute_error(yg, yb)\n",
    "    wmape_base = np.sum(np.abs(yg - yb)) / np.sum(np.abs(yg)) if np.sum(np.abs(yg)) > 0 else np.nan\n",
    "\n",
    "    results.append({\n",
    "        'ASIN': asin,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE(%)': mape,\n",
    "        'WMAPE': wmape,\n",
    "        'MAE_naive': mae_base,\n",
    "        'WMAPE_naive': wmape_base,\n",
    "        'n_test_rows': len(g)\n",
    "    })\n",
    "\n",
    "df_holdout_eval = pd.DataFrame(results).sort_values('WMAPE')\n",
    "\n",
    "# Summary metrics\n",
    "median_wmape = df_holdout_eval['WMAPE'].median()\n",
    "q75_wmape = df_holdout_eval['WMAPE'].quantile(0.75)\n",
    "frac_under_10 = (df_holdout_eval['WMAPE'] < 0.10).mean()\n",
    "frac_under_20 = (df_holdout_eval['WMAPE'] < 0.20).mean()\n",
    "frac_improved_over_naive = (df_holdout_eval['WMAPE'] < df_holdout_eval['WMAPE_naive']).mean()\n",
    "\n",
    "print('\\nHoldout summary:')\n",
    "print(f\"ASINs evaluated: {len(df_holdout_eval)}\")\n",
    "print(f\"Median WMAPE: {median_wmape:.4f}\")\n",
    "print(f\"75th percentile WMAPE: {q75_wmape:.4f}\")\n",
    "print(f\"Fraction ASINs WMAPE <10%: {frac_under_10:.2%}\")\n",
    "print(f\"Fraction ASINs WMAPE <20%: {frac_under_20:.2%}\")\n",
    "print(f\"Fraction improved over naive baseline: {frac_improved_over_naive:.2%}\")\n",
    "\n",
    "# Save results\n",
    "out_path = 'holdout_eval.csv'\n",
    "df_holdout_eval.to_csv(out_path, index=False)\n",
    "print(f\"Saved per-ASIN holdout evaluation to {out_path}\")\n",
    "\n",
    "# Display top/bottom examples\n",
    "print('\\nTop 5 ASINs by best WMAPE:')\n",
    "print(df_holdout_eval.head().to_string(index=False))\n",
    "print('\\nTop 5 ASINs by worst WMAPE:')\n",
    "print(df_holdout_eval.tail().to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
